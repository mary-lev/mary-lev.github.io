<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en, ru"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://mary-lev.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mary-lev.github.io/" rel="alternate" type="text/html" hreflang="en, ru"/><updated>2025-06-22T12:47:10+00:00</updated><id>https://mary-lev.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">TEI Encoding as a Unified Structure for Multilingual Digital Editions</title><link href="https://mary-lev.github.io/blog/2025/aiucd-tei-encoding/" rel="alternate" type="text/html" title="TEI Encoding as a Unified Structure for Multilingual Digital Editions"/><published>2025-06-11T09:00:00+00:00</published><updated>2025-06-11T09:00:00+00:00</updated><id>https://mary-lev.github.io/blog/2025/aiucd-tei-encoding</id><content type="html" xml:base="https://mary-lev.github.io/blog/2025/aiucd-tei-encoding/"><![CDATA[<p>I presented the LeggoManzoni case study at AIUCD-2025 (The 14th Annual Conference of the Association of Digital Humanities and Digital Culture) in Verona, Italy. This presentation, co-authored with Beatrice Nava and Ersilia Russo, demonstrated how TEI encoding can serve as a unified structure for multilingual digital editions.</p> <h2 id="research-context">Research Context</h2> <p>The presentation addressed a key challenge in digital scholarly editing: how to create coherent, interoperable multilingual editions that maintain scholarly standards while enabling computational analysis and user interaction.</p> <h2 id="the-leggomanzoni-project">The LeggoManzoni Project</h2> <p>LeggoManzoni is a comprehensive digital edition project centered on Alessandro Manzoni’s “I Promessi Sposi” (The Betrothed), featuring:</p> <ul> <li><strong>Multiple language versions</strong>: Italian original and various translations</li> <li><strong>Critical apparatus</strong>: Scholarly annotations and commentary</li> <li><strong>Interactive features</strong>: Enhanced reading experience with digital tools</li> <li><strong>Educational focus</strong>: Resources for teaching Italian literature</li> </ul> <h2 id="tei-implementation-strategy">TEI Implementation Strategy</h2> <p>Our approach demonstrates how TEI encoding can:</p> <h3 id="unify-multiple-texts">Unify Multiple Texts</h3> <ul> <li><strong>Parallel structure</strong>: Consistent encoding across different language versions</li> <li><strong>Alignment markers</strong>: Linking corresponding passages between texts</li> <li><strong>Hierarchical organization</strong>: Maintaining textual relationships at multiple levels</li> </ul> <h3 id="support-multilingual-features">Support Multilingual Features</h3> <ul> <li><strong>Language identification</strong>: Proper markup for different languages and dialects</li> <li><strong>Translation relationships</strong>: Explicit connections between source and target texts</li> <li><strong>Cultural annotations</strong>: Context-sensitive explanatory materials</li> </ul> <h3 id="enable-computational-analysis">Enable Computational Analysis</h3> <ul> <li><strong>Structured data</strong>: Machine-readable format for text mining and analysis</li> <li><strong>Standardized markup</strong>: Interoperability with digital humanities tools</li> <li><strong>Extensible framework</strong>: Capacity for future enhancements and additions</li> </ul> <h2 id="technical-innovations">Technical Innovations</h2> <p>The project showcases several advances in TEI practice:</p> <ul> <li><strong>Modular design</strong>: Reusable components for different texts and languages</li> <li><strong>Validation frameworks</strong>: Quality assurance for complex multilingual markup</li> <li><strong>Publishing workflows</strong>: From TEI source to multiple output formats</li> </ul> <h2 id="broader-implications">Broader Implications</h2> <p>This work contributes to:</p> <ul> <li><strong>Digital edition methodology</strong>: Best practices for multilingual projects</li> <li><strong>TEI development</strong>: Extending standards for complex use cases</li> <li><strong>Cultural heritage</strong>: Preserving and presenting literary works digitally</li> <li><strong>Educational technology</strong>: Tools for literature teaching and learning</li> </ul> <h2 id="publication">Publication</h2> <p>The full paper is available with detailed technical specifications and case study analysis, providing a replicable model for similar multilingual digital edition projects.</p> <p>The LeggoManzoni project exemplifies how rigorous encoding standards can support both scholarly research and public engagement with literary heritage.</p>]]></content><author><name></name></author><category term="conferences"/><category term="digital-editions"/><category term="TEI"/><category term="digital-editions"/><category term="multilingual-texts"/><category term="XML"/><category term="encoding"/><category term="LeggoManzoni"/><summary type="html"><![CDATA[AIUCD-2025, Verona, Italy]]></summary></entry><entry><title type="html">Subject Indexing, Chatbot, and Computational Text Analysis in Zemelah.online</title><link href="https://mary-lev.github.io/blog/2025/israeli-dh-zemelah-archive/" rel="alternate" type="text/html" title="Subject Indexing, Chatbot, and Computational Text Analysis in Zemelah.online"/><published>2025-06-10T09:00:00+00:00</published><updated>2025-06-10T09:00:00+00:00</updated><id>https://mary-lev.github.io/blog/2025/israeli-dh-zemelah-archive</id><content type="html" xml:base="https://mary-lev.github.io/blog/2025/israeli-dh-zemelah-archive/"><![CDATA[<p>I presented my work on the Zemelah.online digital archive at the Israeli International Conference on Digital Humanities and Social Sciences at The Open University of Israel in Ra’anana. The presentation focused on advanced digital methods for organizing and accessing Soviet-era Jewish egodocuments.</p> <h2 id="project-overview">Project Overview</h2> <p>Zemelah.online is a digital archive of Soviet-era Jewish egodocuments that combines traditional archival methods with cutting-edge computational approaches to enhance discoverability and user interaction with historical materials.</p> <h2 id="technical-components">Technical Components</h2> <p>The project integrates three main computational approaches:</p> <h3 id="subject-indexing">Subject Indexing</h3> <ul> <li><strong>Automated classification</strong>: Using NLP techniques to categorize documents by topic</li> <li><strong>Semantic tagging</strong>: Identifying key themes and concepts across the collection</li> <li><strong>Multilingual processing</strong>: Handling documents in multiple languages (Russian, Yiddish, Hebrew)</li> </ul> <h3 id="chatbot-interface">Chatbot Interface</h3> <ul> <li><strong>Natural language queries</strong>: Allowing users to ask questions about the archive in natural language</li> <li><strong>Context-aware responses</strong>: Providing relevant document suggestions based on user interests</li> <li><strong>Educational support</strong>: Helping researchers and students navigate the collection</li> </ul> <h3 id="computational-text-analysis">Computational Text Analysis</h3> <ul> <li><strong>Named entity recognition</strong>: Identifying people, places, and organizations</li> <li><strong>Temporal analysis</strong>: Tracking changes in themes and language over time</li> <li><strong>Network analysis</strong>: Mapping relationships between documents and entities</li> </ul> <h2 id="research-significance">Research Significance</h2> <p>This work addresses several important challenges:</p> <ul> <li><strong>Accessibility</strong>: Making historical documents more discoverable</li> <li><strong>Preservation</strong>: Ensuring cultural heritage materials remain accessible</li> <li><strong>Research support</strong>: Providing tools for scholars studying Soviet Jewish history</li> <li><strong>Public engagement</strong>: Making archives accessible to broader audiences</li> </ul> <h2 id="methodological-innovation">Methodological Innovation</h2> <p>The project demonstrates how:</p> <ul> <li>AI can enhance traditional archival practice</li> <li>Computational methods can support humanities research</li> <li>Digital tools can preserve and present cultural heritage</li> <li>Interdisciplinary collaboration enriches both fields</li> </ul> <h2 id="impact">Impact</h2> <p>The Zemelah.online project serves as a model for:</p> <ul> <li>Digital archive development</li> <li>Computational humanities applications</li> <li>Cultural heritage preservation</li> <li>Public history initiatives</li> </ul> <p>This work contributes to the broader field of digital humanities by showing how advanced computational methods can make historical collections more accessible while preserving their scholarly integrity.</p>]]></content><author><name></name></author><category term="conferences"/><category term="digital-archives"/><category term="digital-archives"/><category term="jewish-studies"/><category term="computational-text-analysis"/><category term="chatbot"/><category term="NLP"/><summary type="html"><![CDATA[Israeli International Conference on Digital Humanities and Social Sciences, Ra'anana, Israel]]></summary></entry><entry><title type="html">Literary Communities in Saint Petersburg 1999-2019</title><link href="https://mary-lev.github.io/blog/2025/graphs-networks-literary-communities/" rel="alternate" type="text/html" title="Literary Communities in Saint Petersburg 1999-2019"/><published>2025-02-13T09:00:00+00:00</published><updated>2025-02-13T09:00:00+00:00</updated><id>https://mary-lev.github.io/blog/2025/graphs-networks-literary-communities</id><content type="html" xml:base="https://mary-lev.github.io/blog/2025/graphs-networks-literary-communities/"><![CDATA[<p>I presented my research on literary communities at the Graphs &amp; Networks Conference 2025 at Università della Svizzera italiana in Mendrisio, Switzerland. This presentation explored the application of graph theory and network analysis to understand literary community formation and evolution.</p> <h2 id="conference-context">Conference Context</h2> <p>The Graphs &amp; Networks Conference brings together researchers from various disciplines who apply network analysis methods to complex systems. This interdisciplinary venue was ideal for presenting literary network analysis to a broader scientific community.</p> <h2 id="research-overview">Research Overview</h2> <p>The study examined how literary communities in Saint Petersburg formed, evolved, and dissolved over a 20-year period (1999-2019) using graph-theoretic approaches applied to cultural data.</p> <h2 id="network-analysis-methods">Network Analysis Methods</h2> <p>The research employed several network analysis techniques:</p> <ul> <li><strong>Community detection algorithms</strong>: Identifying clusters of closely connected literary actors</li> <li><strong>Centrality measures</strong>: Determining influential nodes in the literary network</li> <li><strong>Temporal network analysis</strong>: Tracking changes in community structure over time</li> <li><strong>Homophily analysis</strong>: Examining how similar actors tend to connect</li> </ul> <h2 id="key-findings">Key Findings</h2> <p>The analysis revealed:</p> <ul> <li><strong>Distinct literary communities</strong>: Separate clusters based on genre, generation, and aesthetic preferences</li> <li><strong>Bridge figures</strong>: Individuals who connected different literary circles</li> <li><strong>Temporal dynamics</strong>: How communities merged, split, and reformed over time</li> <li><strong>Structural patterns</strong>: Common organizational principles in literary networks</li> </ul> <h2 id="methodological-innovation">Methodological Innovation</h2> <p>This work demonstrates how:</p> <ul> <li>Graph theory can illuminate cultural phenomena</li> <li>Literary data can be analyzed using network science methods</li> <li>Temporal analysis reveals community evolution patterns</li> <li>Interdisciplinary approaches enrich literary studies</li> </ul> <h2 id="broader-implications">Broader Implications</h2> <p>The research contributes to:</p> <ul> <li>Understanding cultural network dynamics</li> <li>Developing methods for analyzing literary communities</li> <li>Bridging humanities and network science</li> <li>Providing tools for cultural policy and literary history</li> </ul> <p>The work showcases how computational methods can offer new insights into traditional humanities questions while respecting the complexity of cultural phenomena.</p>]]></content><author><name></name></author><category term="conferences"/><category term="network-analysis"/><category term="network-analysis"/><category term="literary-communities"/><category term="graph-theory"/><category term="Saint-Petersburg"/><summary type="html"><![CDATA[Graphs & Networks Conference 2025, Università della Svizzera italiana, Mendrisio, Switzerland]]></summary></entry><entry><title type="html">St. Petersburg Literary Network (based on the SPbLitGuide newsletters)</title><link href="https://mary-lev.github.io/blog/2025/dh-russia-spb-network/" rel="alternate" type="text/html" title="St. Petersburg Literary Network (based on the SPbLitGuide newsletters)"/><published>2025-01-31T09:00:00+00:00</published><updated>2025-01-31T09:00:00+00:00</updated><id>https://mary-lev.github.io/blog/2025/dh-russia-spb-network</id><content type="html" xml:base="https://mary-lev.github.io/blog/2025/dh-russia-spb-network/"><![CDATA[<p>I presented my research on the St. Petersburg literary network at the DH-Russia National Research and Project Seminar “Digital Humanities Projects: Models, Problems, Prospects” in Moscow. This presentation focused on the social network analysis of literary communities based on data from SPbLitGuide newsletters.</p> <h2 id="research-background">Research Background</h2> <p>The SPbLitGuide newsletters documented literary events in Saint Petersburg from 1999 to 2019, providing a unique dataset for understanding the evolution of the city’s literary community over two decades.</p> <h2 id="network-analysis-approach">Network Analysis Approach</h2> <p>The research employed social network analysis techniques to:</p> <ul> <li><strong>Map literary connections</strong>: Identify relationships between authors, venues, and cultural institutions</li> <li><strong>Analyze community structure</strong>: Detect clusters and subgroups within the literary network</li> <li><strong>Track temporal evolution</strong>: Examine how the network changed over the 20-year period</li> <li><strong>Identify key actors</strong>: Determine influential figures and venues in the literary landscape</li> </ul> <h2 id="key-findings">Key Findings</h2> <p>The analysis revealed:</p> <ul> <li>Central venues that served as hubs for literary activity</li> <li>Distinct literary communities and their interconnections</li> <li>Temporal patterns in network formation and dissolution</li> <li>The role of specific events in connecting different literary circles</li> </ul> <h2 id="methodological-contributions">Methodological Contributions</h2> <p>This work demonstrates how digital humanities approaches can:</p> <ul> <li>Transform archival materials into structured data for analysis</li> <li>Apply network science methods to literary history</li> <li>Provide quantitative insights into cultural phenomena</li> <li>Support traditional literary scholarship with computational methods</li> </ul> <h2 id="data-availability">Data Availability</h2> <p>The underlying dataset is available on Zenodo: <a href="https://doi.org/10.5281/zenodo.10086515">Literary Events in Saint Petersburg (1999-2019)</a></p> <h2 id="impact">Impact</h2> <p>This research contributes to understanding Russian literary communities and demonstrates the potential of digital methods for cultural analysis, offering new perspectives on the social dynamics of literary production and distribution.</p>]]></content><author><name></name></author><category term="conferences"/><category term="digital-humanities"/><category term="social-network-analysis"/><category term="russian-literature"/><category term="Saint-Petersburg"/><category term="digital-humanities"/><summary type="html"><![CDATA[DH-Russia National Research and Project Seminar, Moscow, Russia]]></summary></entry><entry><title type="html">Automatic Translation Alignment Pipeline for Multilingual Digital Editions of Literary Works</title><link href="https://mary-lev.github.io/blog/2024/chr2024-translation-pipeline/" rel="alternate" type="text/html" title="Automatic Translation Alignment Pipeline for Multilingual Digital Editions of Literary Works"/><published>2024-12-04T09:00:00+00:00</published><updated>2024-12-04T09:00:00+00:00</updated><id>https://mary-lev.github.io/blog/2024/chr2024-translation-pipeline</id><content type="html" xml:base="https://mary-lev.github.io/blog/2024/chr2024-translation-pipeline/"><![CDATA[<p>I presented my research on automatic translation alignment at the Computational Humanities Research Conference 2024 at Aarhus University in Denmark. This presentation detailed the development of an automated pipeline for aligning translations in multilingual digital editions.</p> <h2 id="research-overview">Research Overview</h2> <p>The project addresses a critical challenge in digital humanities: how to automatically align texts across multiple languages and editions to create comprehensive multilingual digital scholarly editions.</p> <h2 id="technical-approach">Technical Approach</h2> <p>The pipeline integrates several computational methods:</p> <ul> <li><strong>Text preprocessing</strong>: Normalization and segmentation of source and target texts</li> <li><strong>Alignment algorithms</strong>: Advanced techniques for identifying corresponding text segments</li> <li><strong>Quality assessment</strong>: Automated evaluation of alignment accuracy</li> <li><strong>TEI integration</strong>: Export of aligned texts in TEI-compliant XML format</li> </ul> <h2 id="case-study-i-promessi-sposi">Case Study: I Promessi Sposi</h2> <p>The methodology was tested on Alessandro Manzoni’s “I Promessi Sposi” and its translations, demonstrating:</p> <ul> <li>Effectiveness across different language pairs</li> <li>Handling of structural variations between editions</li> <li>Integration with existing digital edition frameworks</li> </ul> <h2 id="key-contributions">Key Contributions</h2> <ul> <li>Development of a robust automatic alignment pipeline</li> <li>Evaluation metrics for translation alignment quality</li> <li>Integration strategies for multilingual TEI documents</li> <li>Practical solutions for common alignment challenges</li> </ul> <h2 id="publication">Publication</h2> <p>The full paper is available in the CHR 2024 Proceedings: <a href="https://ceur-ws.org/Vol-3834/paper128.pdf">CEUR-WS Vol-3834, pp. 1086-1104</a></p> <h2 id="impact">Impact</h2> <p>This work contributes to making multilingual digital editions more accessible and scalable, reducing the manual effort required for creating aligned translations while maintaining scholarly standards for digital textual scholarship.</p> <p>The pipeline has been applied to the LeggoManzoni project and is being adapted for other multilingual digital edition projects.</p>]]></content><author><name></name></author><category term="conferences"/><category term="computational-humanities"/><category term="translation-alignment"/><category term="computational-humanities"/><category term="multilingual-editions"/><category term="NLP"/><summary type="html"><![CDATA[Computational Humanities Research 2024, Aarhus University, Denmark]]></summary></entry><entry><title type="html">СтихиРу vs поэтический канон: к вопросу о количественном измерении графомании</title><link href="https://mary-lev.github.io/blog/2024/grafomaniya-conference/" rel="alternate" type="text/html" title="СтихиРу vs поэтический канон: к вопросу о количественном измерении графомании"/><published>2024-10-07T09:00:00+00:00</published><updated>2024-10-07T09:00:00+00:00</updated><id>https://mary-lev.github.io/blog/2024/grafomaniya-conference</id><content type="html" xml:base="https://mary-lev.github.io/blog/2024/grafomaniya-conference/"><![CDATA[<p>I presented my research on quantitative approaches to analyzing Russian poetry at the conference “Графомания. Русская литература и ее границы” (Graphomania. Russian Literature and Its Boundaries) at the Institute of Russian Literature (Pushkinskij Dom) in Saint Petersburg.</p> <h2 id="research-question">Research Question</h2> <p>The presentation addressed the question of how computational methods can help distinguish between canonical poetry and what might be considered “graphomania” - examining whether quantitative measures can capture qualitative differences in literary value.</p> <h2 id="methodology">Methodology</h2> <p>The study compared texts from Стихи.ру (Stihi.ru), a popular Russian poetry platform, with established poetic canon using various computational metrics:</p> <ul> <li><strong>Linguistic features</strong>: Vocabulary diversity, syntactic complexity, rhythmic patterns</li> <li><strong>Statistical analysis</strong>: Frequency distributions, stylometric measures</li> <li><strong>Comparative analysis</strong>: Canonical vs. non-canonical poetry characteristics</li> </ul> <h2 id="key-findings">Key Findings</h2> <p>The research explored whether computational methods can identify markers that distinguish between:</p> <ul> <li>Canonical Russian poetry from the literary tradition</li> <li>Contemporary poetry published on digital platforms</li> <li>The boundaries and overlaps between these categories</li> </ul> <h2 id="theoretical-implications">Theoretical Implications</h2> <p>The presentation examined the broader theoretical questions around:</p> <ul> <li>Digital humanities approaches to literary value assessment</li> <li>The role of computational methods in literary criticism</li> <li>Challenges in defining and measuring literary quality</li> </ul> <h2 id="conference-context">Conference Context</h2> <p>This conference brought together scholars examining the boundaries of Russian literature, providing an important forum for discussing how digital methods can inform traditional literary studies while respecting the complexity of aesthetic judgment.</p>]]></content><author><name></name></author><category term="conferences"/><category term="russian-literature"/><category term="russian-poetry"/><category term="computational-analysis"/><category term="literary-canon"/><category term="digital-humanities"/><summary type="html"><![CDATA[Графомания. Русская литература и ее границы, Institute of Russian Literature, Saint Petersburg]]></summary></entry><entry><title type="html">Manual and Automatic Alignment of I Promessi Sposi: Results and Open Issues</title><link href="https://mary-lev.github.io/blog/2024/roma-translation-alignment/" rel="alternate" type="text/html" title="Manual and Automatic Alignment of I Promessi Sposi: Results and Open Issues"/><published>2024-09-26T09:00:00+00:00</published><updated>2024-09-26T09:00:00+00:00</updated><id>https://mary-lev.github.io/blog/2024/roma-translation-alignment</id><content type="html" xml:base="https://mary-lev.github.io/blog/2024/roma-translation-alignment/"><![CDATA[<p>I participated in the workshop “Aligned Translations for Digital Scholarly Editions: Methodologies, Methods, and Workflows” at the Istituto Italiano di Studi Germanici in Rome, presenting my work on the alignment challenges in multilingual digital editions.</p> <h2 id="presentation-focus">Presentation Focus</h2> <p>The presentation examined both manual and automatic approaches to aligning different versions and translations of Alessandro Manzoni’s “I Promessi Sposi” (The Betrothed), discussing the practical challenges and theoretical implications of creating aligned multilingual digital editions.</p> <h2 id="key-topics-covered">Key Topics Covered</h2> <ul> <li><strong>Manual alignment workflows</strong>: Traditional philological approaches adapted for digital environments</li> <li><strong>Automatic alignment pipelines</strong>: Computational methods for text alignment across languages and editions</li> <li><strong>Quality assessment</strong>: Comparing manual and automatic alignment results</li> <li><strong>Open challenges</strong>: Remaining issues in multilingual digital edition creation</li> </ul> <h2 id="technical-challenges">Technical Challenges</h2> <p>The work highlighted several persistent challenges in translation alignment:</p> <ul> <li>Handling different text structures across languages</li> <li>Managing varying levels of textual correspondence</li> <li>Balancing accuracy with processing efficiency</li> <li>Integrating alignment data into TEI-compliant formats</li> </ul> <h2 id="workshop-context">Workshop Context</h2> <p>This workshop brought together scholars working on multilingual digital editions, providing a valuable forum for discussing methodological approaches and sharing practical solutions for common challenges in the field.</p> <p>The research contributes to the broader LeggoManzoni project, which aims to create comprehensive multilingual digital editions with integrated critical apparatus.</p>]]></content><author><name></name></author><category term="conferences"/><category term="digital-editions"/><category term="translation-alignment"/><category term="digital-editions"/><category term="TEI"/><category term="multilingual-texts"/><summary type="html"><![CDATA[Aligned Translations for Digital Scholarly Editions, Roma, Italy]]></summary></entry><entry><title type="html">Mapping Literary Space: A Social Network from the Timeline of Cultural Events</title><link href="https://mary-lev.github.io/blog/2024/dh2024-mapping-literary-space/" rel="alternate" type="text/html" title="Mapping Literary Space: A Social Network from the Timeline of Cultural Events"/><published>2024-08-06T09:00:00+00:00</published><updated>2024-08-06T09:00:00+00:00</updated><id>https://mary-lev.github.io/blog/2024/dh2024-mapping-literary-space</id><content type="html" xml:base="https://mary-lev.github.io/blog/2024/dh2024-mapping-literary-space/"><![CDATA[<p>I presented my research on literary communities at the ADHO Digital Humanities Conference 2024 at George Mason University in Washington, DC. The presentation focused on mapping the social literary network in Saint Petersburg from 1999 to 2019 using social network analysis within the digital humanities framework.</p> <h2 id="research-overview">Research Overview</h2> <p>This study analyzes the cultural landscape of Saint Petersburg through the lens of social network analysis, examining how literary events and cultural activities create connections between writers, critics, and cultural institutions over two decades.</p> <h2 id="methodology">Methodology</h2> <p>The research is based on data extracted from SPbLitGuide newsletters, which documented literary events in Saint Petersburg from 1999 to 2019. Using network analysis techniques, I mapped the relationships between:</p> <ul> <li>Literary venues and institutions</li> <li>Authors and their collaborative networks</li> <li>Cultural events and their participants</li> <li>Temporal patterns in literary community formation</li> </ul> <h2 id="key-findings">Key Findings</h2> <p>The analysis revealed distinct patterns in how Saint Petersburg’s literary community evolved over the studied period, highlighting the role of specific venues, events, and individuals in shaping the city’s cultural landscape.</p> <h2 id="publication">Publication</h2> <p>The full abstract is available in the DH2024 Book of Abstracts: <a href="https://doi.org/10.5281/zenodo.13761079">https://doi.org/10.5281/zenodo.13761079</a></p> <p>The underlying dataset is published on Zenodo: <a href="https://doi.org/10.5281/zenodo.10086515">Literary Events in Saint Petersburg (1999-2019)</a></p>]]></content><author><name></name></author><category term="conferences"/><category term="digital-humanities"/><category term="social-network-analysis"/><category term="digital-humanities"/><category term="literary-studies"/><category term="Saint-Petersburg"/><summary type="html"><![CDATA[ADHO Digital Humanities Conference 2024, George Mason University, Washington, DC]]></summary></entry><entry><title type="html">Handwritten text recongition slides</title><link href="https://mary-lev.github.io/blog/2024/handwrittten-text-recognition/" rel="alternate" type="text/html" title="Handwritten text recongition slides"/><published>2024-04-30T13:56:00+00:00</published><updated>2024-04-30T13:56:00+00:00</updated><id>https://mary-lev.github.io/blog/2024/handwrittten-text-recognition</id><content type="html" xml:base="https://mary-lev.github.io/blog/2024/handwrittten-text-recognition/"><![CDATA[<p><img src="https://raw.githubusercontent.com/mary-lev/mary-lev.github.io/master/assets/img/HTR.jpg" alt="Handwritten Text Recognition"/></p> <p>I recently presented at the University of Bologna’s seminar on “Machine Learning for Art and Humanities.” My presentation focused on handwritten text recognition (HTR), going over the HTR pipeline and looking at what’s currently being done in this area.</p> <p>Here are <a href="https://colab.research.google.com/drive/1aEcRFIMhzlfjy3Yq-o9tc-Gi-IPLwHsK?usp=sharing">two main HTR pipeline steps simplified with Google Colab</a>, <a href="https://github.com/mary-lev/HTR_demo">some data</a> and <a href="https://www.slideshare.net/slideshow/handwritten-text-recognition-for-manuscripts-and-early-printed-texts/267679039">the slides</a>.</p>]]></content><author><name></name></author><category term="ML,"/><category term="talks"/><category term="HTR,"/><category term="ATR,"/><category term="transformers,"/><category term="YOLO,"/><category term="TrOCR,"/><category term="manuscripts"/><summary type="html"><![CDATA[HTR presentation]]></summary></entry><entry><title type="html">My first Kaggle competition</title><link href="https://mary-lev.github.io/blog/2024/kaggle_competition/" rel="alternate" type="text/html" title="My first Kaggle competition"/><published>2024-01-07T13:56:00+00:00</published><updated>2024-01-07T13:56:00+00:00</updated><id>https://mary-lev.github.io/blog/2024/kaggle_competition</id><content type="html" xml:base="https://mary-lev.github.io/blog/2024/kaggle_competition/"><![CDATA[<p>It was <a href="https://www.kaggle.com/competitions/news-scraping-competition/overview">my first kaggle competition</a>, and I have won it!</p> <p>This fairly simple news classification competition was run by the Faculty of Computer Science (FCS) at the Higher School of Economics (HSE) as part of their bootcamp. What made this win special was how I achieved it: through a simple yet effective approach.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/19-480.webp 480w,/assets/img/19-800.webp 800w,/assets/img/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/19.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Leaderboard. </div> <h2 id="the-challenge">The Challenge</h2> <p>The task was to predict news topics. We were to categorize news as ‘Society/Russia’, ‘Science and Technology’, and other types. Our unique idea was to accumulate training data by scraping news sites. To solve this problem, I first carefully probed the test data. This exploration was very important to selecting which specific data to analyze. As a result, I collected material for three years from Lenta. Ru and four years from Fontanka. Ru.</p> <h2 id="complexity-or-simplicity">Complexity or Simplicity?</h2> <p>At first, I thought about using a less complex model like CatBoost or XGBoost,even went the route of testing out a bit of advanced transformers. I didn’t get what I wanted from any of these That took me to the next phase, one in which I decided to rely on the simple side. Using basic techniques like CountVectorizer and TfidfVectorizer, combined with Logistic Regression, I made not only a model that was efficient but amazingly more effective than complex alternatives.</p> <h2 id="the-results-a-clear-victory-for-simplicity">The Results: A Clear Victory for Simplicity</h2> <p>The approach was successful, with accuracy scores of 0.92 on CountVectorizer and 0.93 on TfidfVectorizer. The final results of this competition were particularly impressive. The public accuracy was 0.98548 and the private score was 0.98756. These numbers demonstrate that carefully-made simple approaches can surpass more intricate ones in efficiency.</p> <p>In the spirit of community and collaboration, I’ve uploaded <a href="https://www.kaggle.com/marialevchenko/datasets">both datasets used in this competition to Kaggle</a>. I hope that these resources will help future explorers in their data science pursuits.</p> <h2 id="exploring-further-topic-modeling-in-russian-news">Exploring Further: Topic Modeling in Russian News</h2> <p>The competition inspired me to explore topic modeling within Russian news texts. I was motivated by my work on news classification. I hope to uncover how subjects and lexicon in Russian news have evolved over the last three years. The purpose of this journey into topic modeling is to gain deeper insights into the shifts in societal and cultural narratives as reflected in the media. This project is ongoing, and I’ll keep you updated with my findings as they unfold.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/news-classification.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="competitions,"/><category term="awards"/><category term="kaggle,"/><category term="classification,"/><category term="news,"/><category term="Russian"/><summary type="html"><![CDATA[News classification for Russian]]></summary></entry></feed>