<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en, ru"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://mary-lev.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mary-lev.github.io/" rel="alternate" type="text/html" hreflang="en, ru"/><updated>2024-01-16T04:11:03+00:00</updated><id>https://mary-lev.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">My first Kaggle competition</title><link href="https://mary-lev.github.io/blog/2024/kaggle_competition/" rel="alternate" type="text/html" title="My first Kaggle competition"/><published>2024-01-07T13:56:00+00:00</published><updated>2024-01-07T13:56:00+00:00</updated><id>https://mary-lev.github.io/blog/2024/kaggle_competition</id><content type="html" xml:base="https://mary-lev.github.io/blog/2024/kaggle_competition/"><![CDATA[<p>It was <a href="https://www.kaggle.com/competitions/news-scraping-competition/overview">my first kaggle competition</a>, and I have won it!</p> <p>This fairly simple news classification competition was run by the Faculty of Computer Science (FCS) at the Higher School of Economics (HSE) as part of their bootcamp. What made this win special was how I achieved it: through a simple yet effective approach.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/19-480.webp 480w,/assets/img/19-800.webp 800w,/assets/img/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/19.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Leaderboard. </div> <h2 id="the-challenge">The Challenge</h2> <p>The task was to predict news topics. We were to categorize news as ‘Society/Russia’, ‘Science and Technology’, and other types. Our unique idea was to accumulate training data by scraping news sites. To solve this problem, I first carefully probed the test data. This exploration was very important to selecting which specific data to analyze. As a result, I collected material for three years from Lenta. Ru and four years from Fontanka. Ru.</p> <h2 id="complexity-or-simplicity">Complexity or Simplicity?</h2> <p>At first, I thought about using a less complex model like CatBoost or XGBoost,even went the route of testing out a bit of advanced transformers. I didn’t get what I wanted from any of these That took me to the next phase, one in which I decided to rely on the simple side. Using basic techniques like CountVectorizer and TfidfVectorizer, combined with Logistic Regression, I made not only a model that was efficient but amazingly more effective than complex alternatives.</p> <h2 id="the-results-a-clear-victory-for-simplicity">The Results: A Clear Victory for Simplicity</h2> <p>The approach was successful, with accuracy scores of 0.92 on CountVectorizer and 0.93 on TfidfVectorizer. The final results of this competition were particularly impressive. The public accuracy was 0.98548 and the private score was 0.98756. These numbers demonstrate that carefully-made simple approaches can surpass more intricate ones in efficiency.</p> <p>In the spirit of community and collaboration, I’ve uploaded <a href="https://www.kaggle.com/marialevchenko/datasets">both datasets used in this competition to Kaggle</a>. I hope that these resources will help future explorers in their data science pursuits.</p> <h2 id="exploring-further-topic-modeling-in-russian-news">Exploring Further: Topic Modeling in Russian News</h2> <p>The competition inspired me to explore topic modeling within Russian news texts. I was motivated by my work on news classification. I hope to uncover how subjects and lexicon in Russian news have evolved over the last three years. The purpose of this journey into topic modeling is to gain deeper insights into the shifts in societal and cultural narratives as reflected in the media. This project is ongoing, and I’ll keep you updated with my findings as they unfold.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/news-classification.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="competitions,"/><category term="awards"/><category term="kaggle,"/><category term="classification,"/><category term="news,"/><category term="Russian"/><summary type="html"><![CDATA[News classification for Russian]]></summary></entry><entry><title type="html">Finished the article about LeggoManzoni</title><link href="https://mary-lev.github.io/blog/2023/post-bibliography/" rel="alternate" type="text/html" title="Finished the article about LeggoManzoni"/><published>2023-12-25T13:56:00+00:00</published><updated>2023-12-25T13:56:00+00:00</updated><id>https://mary-lev.github.io/blog/2023/post-bibliography</id><content type="html" xml:base="https://mary-lev.github.io/blog/2023/post-bibliography/"><![CDATA[<p>Today marks the completion of an article about our project, which has been both a challenge and a fun time: “Commentare – Leggo Manzoni. Quaranta commenti alla Quarantana.” This article, a collaborative effort with Giulia Menna and Beatrice Nava, is set to be published in 2024.</p> <blockquote><p>Il progetto <em>Leggo Manzoni. Quaranta commenti alla Quarantana</em>, sviluppato in continuità con il Prin <a href="https://www.alessandromanzoni.org">Manzoni online</a> sotto la supervisione scientifica di Paola Italia e Francesca Tomasi, tenta di dare seguito a queste premesse, fornendo a chi insegna uno strumento per leggere in modo più interattivo un grande classico della letteratura italiana, significativamente centro perenne e punto di partenza delle incessanti polemiche sulla necessità di svecchiamento dei programmi e di una ridefinizione del canone letterario ad uso scolastico, ma anche di interessanti progetti di rilettura innovativa.</p><cite><a class="citation" href="#levchenko2024commentare">(Levchenko et al., 2024)</a></cite></blockquote>]]></content><author><name></name></author><category term="Italian,"/><category term="publications"/><category term="LeggoManzoni"/><summary type="html"><![CDATA[Commentare – Leggo Manzoni. Quaranta commenti alla Quarantana]]></summary></entry><entry><title type="html">Data Processing for SPbLitGuide project</title><link href="https://mary-lev.github.io/blog/2023/eml_diagram/" rel="alternate" type="text/html" title="Data Processing for SPbLitGuide project"/><published>2023-10-04T17:39:00+00:00</published><updated>2023-10-04T17:39:00+00:00</updated><id>https://mary-lev.github.io/blog/2023/eml_diagram</id><content type="html" xml:base="https://mary-lev.github.io/blog/2023/eml_diagram/"><![CDATA[<p>The data processing pipeline for the literary events dataset involved several stages, each crucial for transforming raw data into a structured and analyzable format. Here’s a scheme:</p> <h2 id="data-model">Data Model</h2> <pre><code class="language-mermaid">sequenceDiagram
    participant EML as EML files
    participant WP as Wordpress Database
    participant Python as Python
    participant Tidy as Tidy Data
    participant Yandex as Yandex API
    participant AI as AI Enrichment (DeepPavlov &amp; LLM)
    participant Network as Network Graph

    EML-&gt;&gt;WP: Import EML files
    WP-&gt;&gt;Python: Export XML file
    Python-&gt;&gt;Tidy: processing
    Yandex-&gt;&gt;Tidy: geo coordinates
    Tidy--&gt;&gt;AI: NER
    Tidy-&gt;&gt;Network: NetworkX
</code></pre> <ol> <li>Data Sourcing: <ul> <li>Source: Electronic mail (EML) files uploaded to a WordPress site.</li> <li>Content: 1,157 issues of mailing lists containing event details, dates, places, and sources.</li> <li>Format: XML with HTML tags, inconsistent addresses, and typos.</li> </ul> </li> <li>Data Cleaning and Preprocessing: <ul> <li>Regular expressions and filtering were used to remove unrelated sections and HTML tags.</li> <li>The cleaned data was structured into a CSV format with columns for event date, description, source, issue title, permalink, and publication date.</li> <li>Result: 44,930 records of events.</li> </ul> </li> <li>Duplicate Filtering: <ul> <li>Fuzzy matching algorithms identified and eliminated duplicates.</li> <li>Outcome: 14,498 unique events.</li> </ul> </li> <li>Named Entity Recognition (NER): <ul> <li>Natasha library extracted dates, addresses, and places, often marked with HTML tags.</li> <li>DeepPavlov library extracted persons’ names.</li> <li>Pandas and difflib libraries standardized venue names and addresses.</li> <li>Result: A dataset with 14,498 literary events, 862 venues, 817 unique addresses, and a comprehensive list of names.</li> </ul> </li> <li>Geographical Coordinates Extraction: <ul> <li>Yandex Geocoder API extracted geographical coordinates from standardized addresses.</li> <li>Outcome: Latitude and longitude coordinates for each unique address.</li> </ul> </li> </ol> <p>This pipeline effectively transformed a large volume of unstructured data into a clean, structured dataset suitable for further analysis and visualization. The use of various libraries and APIs ensured accurate extraction and standardization of key information, making the dataset a valuable resource for understanding literary events and their geographical distribution.</p>]]></content><author><name></name></author><category term="SPbLitGuide,"/><category term="NLP,"/><category term="NER,"/><category term="network"/><summary type="html"><![CDATA[data pipeline giagram]]></summary></entry><entry><title type="html">riPENSIAMOli - Manzoni in digitale</title><link href="https://mary-lev.github.io/blog/2023/videos/" rel="alternate" type="text/html" title="riPENSIAMOli - Manzoni in digitale"/><published>2023-09-19T21:01:00+00:00</published><updated>2023-09-19T21:01:00+00:00</updated><id>https://mary-lev.github.io/blog/2023/videos</id><content type="html" xml:base="https://mary-lev.github.io/blog/2023/videos/"><![CDATA[<p>Last week at Cubo Gruppo Unipol, together with Giulia Menna and Beatrice Nava, I presented the first release version of Leggo Manzoni, a project developed with the University of Bologna and the Advanced Research Centre for Digital Humanities (DH.arc). Speaking three minutes in Italian was even harder for me than writing all the code for the project in Node.JS.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <iframe src="https://www.youtube.com/watch?v=bCx0rVSbKS0&amp;t=2533s" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"/> </figure> </div> </div>]]></content><author><name></name></author><category term="Italian,"/><category term="talks"/><category term="presentation"/><summary type="html"><![CDATA[Cubo Unipol]]></summary></entry><entry><title type="html">Data Modeling for SPbLitGuide project</title><link href="https://mary-lev.github.io/blog/2021/diagrams/" rel="alternate" type="text/html" title="Data Modeling for SPbLitGuide project"/><published>2021-07-04T17:39:00+00:00</published><updated>2021-07-04T17:39:00+00:00</updated><id>https://mary-lev.github.io/blog/2021/diagrams</id><content type="html" xml:base="https://mary-lev.github.io/blog/2021/diagrams/"><![CDATA[<p>The data model leverages Django’s Object-Relational Mapping (ORM) to map the dataset’s schema to Python classes. The model consists of four main entities: Person, Address, Place, and Event. Below is a description of each entity, their fields, and the relationships among them.</p> <h2 id="data-model">Data Model</h2> <pre><code class="language-mermaid">erDiagram
  Person {
    name CharField
    second_name CharField
    family CharField
    pseudonym CharField
  }
  Address {
    name CharField
    lon DecimalField
    lat DecimalField
  }
  Place {
    name CharField
  }
  Event {
    description TextField
    date DateTimeField
  }
  Person ||--o{ Event : ManyToManyField
  Event ||--|{ Place : ForeignKey
  Event ||--|{ Address : ForeignKey
  Place }o--|| Event : ForeignKey
</code></pre> <h2 id="entities-and-fields">Entities and Fields</h2> <h3 id="person">Person</h3> <ul> <li>name: CharField to store the first name.</li> <li>second_name: CharField to store the middle name.</li> <li>family: CharField to store the last name.</li> <li>pseudonym: CharField to store any pseudonym. The Person class uniquely identifies a person by their name and family.</li> </ul> <h3 id="address">Address</h3> <ul> <li>name: CharField to store the name of the address.</li> <li>lon: DecimalField to store longitude.</li> <li>lat: DecimalField to store latitude.</li> </ul> <h3 id="place">Place</h3> <ul> <li>name: CharField to store the name of the place.</li> </ul> <h3 id="event">Event</h3> <ul> <li>description: TextField for event description.</li> <li>date: DateTimeField for the event date and time.</li> <li>place: ForeignKey to the Place entity.</li> <li>address: ForeignKey to the Address entity.</li> <li>people: ManyToManyField connecting to the Person entity.</li> </ul> <h2 id="relationships">Relationships</h2> <p>A Person can be associated with multiple Events through a ManyToManyField. An Event has one Place and one Address associated with it via ForeignKey relationships. A Place can have multiple associated Addresses through the Events.</p> <h2 id="constraints-and-ordering">Constraints and Ordering</h2> <p>Person is constrained to have a unique combination of name and family. Event entities are ordered by their date.</p> <h2 id="pandas-dataframe">Pandas DataFrame</h2> <p>The data is stored in a single pandas DataFrame with the objective of capturing literary events, their venues, and attendees in St. Petersburg. Unlike a relational database model, the DataFrame consolidates all the information into one table, with each row representing a unique event.</p> <h3 id="fields">Fields</h3> <ul> <li>id: Integer to uniquely identify each event.</li> <li>event_description: String to store the description of the event.</li> <li>address: String for the geographical location of the venue.</li> <li>place_name: String for the name of the place where the event is held.</li> <li>date: DateTime type for when the event is scheduled.</li> <li>latitude: Float for the latitude coordinate of the venue.</li> <li>longitude: Float for the longitude coordinate of the venue.</li> <li>persons: List of Strings to store names of people associated with the event.</li> </ul> <h3 id="characteristics">Characteristics</h3> <p>In-Memory Storage: The DataFrame exists in memory; it’s not a persistent database. Flat Structure: No native support for relationships. All data points are flattened into individual fields. Flexible Schema: Easy to add or remove fields as needed. Indexing: Integer-based indexing is used for quick data retrieval.</p> <h3 id="constraints">Constraints</h3> <p>The id field should be unique to maintain the integrity of each record.</p> <h3 id="data-types">Data Types</h3> <ul> <li>id: Integer</li> <li>event_description: Object (typically String)</li> <li>address: Object (typically String)</li> <li>place_name: Object (typically String)</li> <li>date: DateTime</li> <li>latitude: Float64</li> <li>longitude: Float64</li> <li>persons: Object (typically List of Strings)</li> </ul> <p>The DataFrame allows for efficient querying, filtering, and analytics, albeit without the relationship management and persistence features found in traditional databases.</p>]]></content><author><name></name></author><category term="data"/><category term="model"/><summary type="html"><![CDATA[mermaid diagram]]></summary></entry><entry><title type="html">NER with Deeppavlov models</title><link href="https://mary-lev.github.io/blog/2020/deeppvalov-ner/" rel="alternate" type="text/html" title="NER with Deeppavlov models"/><published>2020-09-10T06:15:37+00:00</published><updated>2020-09-10T06:15:37+00:00</updated><id>https://mary-lev.github.io/blog/2020/deeppvalov-ner</id><content type="html" xml:base="https://mary-lev.github.io/blog/2020/deeppvalov-ner/"><![CDATA[<p>You’ll find this post in your <code class="language-plaintext highlighter-rouge">_posts</code> directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run <code class="language-plaintext highlighter-rouge">jekyll serve</code>, which launches a web server and auto-regenerates your site when a file is updated.</p> <p>Jekyll requires blog post files to be named according to the following format:</p> <p><code class="language-plaintext highlighter-rouge">YEAR-MONTH-DAY-title.MARKUP</code></p> <p>Where <code class="language-plaintext highlighter-rouge">YEAR</code> is a four-digit number, <code class="language-plaintext highlighter-rouge">MONTH</code> and <code class="language-plaintext highlighter-rouge">DAY</code> are both two-digit numbers, and <code class="language-plaintext highlighter-rouge">MARKUP</code> is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.</p> <p>Jekyll also offers powerful support for code snippets:</p> <figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="s2">"Hi, </span><span class="si">#{</span><span class="nb">name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s1">'Tom'</span><span class="p">)</span>
<span class="c1">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure> <p>Check out the <a href="https://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyll’s GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>]]></content><author><name></name></author><category term="experiments"/><category term="NER"/><category term="Russian"/><summary type="html"><![CDATA[You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.]]></summary></entry><entry><title type="html">Talks and Sounds in “Crime and Punishment”</title><link href="https://mary-lev.github.io/blog/2019/crime_and_punsihment/" rel="alternate" type="text/html" title="Talks and Sounds in “Crime and Punishment”"/><published>2019-10-19T17:39:00+00:00</published><updated>2019-10-19T17:39:00+00:00</updated><id>https://mary-lev.github.io/blog/2019/crime_and_punsihment</id><content type="html" xml:base="https://mary-lev.github.io/blog/2019/crime_and_punsihment/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/dostoevsky.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="ipynb,"/><category term="NLP,"/><category term="charts,"/><category term="Dostoevsky"/><summary type="html"><![CDATA[text analysys and visualisation with Python]]></summary></entry><entry><title type="html">Dh In Euspb</title><link href="https://mary-lev.github.io/blog/2017/dh-in-euspb/" rel="alternate" type="text/html" title="Dh In Euspb"/><published>2017-04-13T00:00:00+00:00</published><updated>2017-04-13T00:00:00+00:00</updated><id>https://mary-lev.github.io/blog/2017/dh-in-euspb</id><content type="html" xml:base="https://mary-lev.github.io/blog/2017/dh-in-euspb/"><![CDATA[<h1 id="семинар-по-digital-humanities-в-еуспб">Семинар по Digital Humanities в ЕУСПб</h1> <p>Рассказ о проекте Concordances 1.0 и возможностях автоматической обработки текста для литературоведческих целей</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Семинар по Digital Humanities в ЕУСПб Рассказ о проекте Concordances 1.0 и возможностях автоматической обработки текста для литературоведческих целей]]></summary></entry><entry><title type="html">Comparative Analysis of the Manuscripts of ‘Моление Даниила Заточника’ Based on Vector Proximity</title><link href="https://mary-lev.github.io/blog/2017/pushkinskij-dom/" rel="alternate" type="text/html" title="Comparative Analysis of the Manuscripts of ‘Моление Даниила Заточника’ Based on Vector Proximity"/><published>2017-03-01T06:05:37+00:00</published><updated>2017-03-01T06:05:37+00:00</updated><id>https://mary-lev.github.io/blog/2017/pushkinskij-dom</id><content type="html" xml:base="https://mary-lev.github.io/blog/2017/pushkinskij-dom/"><![CDATA[<p>In a recent presentation at the Department of Old Russian Literature of The Pushkinsky House (Institute of Russian Literature), we presented our project explored a novel vector-based approach to the stemma codicum of “Моление Даниила Заточника”. This project aimed to map the genealogical relationships of its witnesses using computational methods.</p> <p>Elena Ripinskaya, a PhD candidate at the Institute of Russian Literature, provided the foundational material and the project’s aim. She compiled <a href="https://concordance.pythonanywhere.com/test/zatochnik/compare/204/1">a comprehensive table documenting the variations in each line across 11 manuscripts</a>, setting the stage for in-depth textual analysis.</p> <p>My contribution was centered around the application of advanced computational techniques. I employed the word2vec algorithm to transform the manuscripts into semantic vector representations. Subsequently, I utilized KMeans clustering to systematically categorize these manuscripts. This methodology was pivotal in revealing distinct groups based on their textual similarities, thus bringing a new perspective to traditional stemmatological methods.</p> <p><a href="https://concordance.pythonanywhere.com/test/zatochnik/index">Sources’ description</a></p>]]></content><author><name></name></author><category term="NLP"/><category term="experiments"/><category term="talks&amp;projects"/><category term="word2vec"/><category term="stemmata"/><category term="KMeans"/><summary type="html"><![CDATA[The Pushkinsky House (Saint-Petersburg), 1st March 2017]]></summary></entry><entry><title type="html">Semantic Vector Models in Russian Poetic Text Analysis</title><link href="https://mary-lev.github.io/blog/2015/seminar-helsinki/" rel="alternate" type="text/html" title="Semantic Vector Models in Russian Poetic Text Analysis"/><published>2015-08-27T05:05:37+00:00</published><updated>2015-08-27T05:05:37+00:00</updated><id>https://mary-lev.github.io/blog/2015/seminar-helsinki</id><content type="html" xml:base="https://mary-lev.github.io/blog/2015/seminar-helsinki/"><![CDATA[<p>This presentation, delivered at the Quantitative Study on Russian Language seminar in Helsinki in August 2015, detailed a research project that employed semantic vector models to examine Russian poetic language. The focus was on identifying distinct linguistic features within a corpus of Russian poetry.</p> <h2 id="objective">Objective:</h2> <p>The primary objective was to conduct a comparative linguistic analysis between a specifically curated corpus of Russian poetic texts and a standard Russian language model, as represented by the RusVectores2.0 project.</p> <h2 id="methodology">Methodology:</h2> <ul> <li>Analysis was based on a corpus comprising 10,000 Russian poetic texts from the period 1890-1920.</li> <li>A vector model, constructed via the word2vec tool, was exclusively developed for this poetic corpus.</li> <li>Comparative analysis with the RusVectores2.0 general Russian language model was undertaken to isolate and identify distinctive poetic linguistic patterns.</li> </ul> <h2 id="findings">Findings:</h2> <p>The comparative analysis elucidated unique semantic attributes inherent to Russian poetic texts. These attributes were markedly distinct from those found in the broader, general Russian language model.</p> <h2 id="conclusion">Conclusion:</h2> <p>This study lays the groundwork for subsequent research in digital linguistics and poetic analysis. It underscores the efficacy of semantic vector models in examining the intricate differences between poetic and standard language usage, opening potential pathways for future linguistic inquiry.</p> <p>To view the complete presentation, please follow <a href="https://docs.google.com/presentation/d/1ujBSKL6qnyaqkM96QjI4V5p50ItgOHsTwWJblvFH6P8">this link</a>.</p>]]></content><author><name></name></author><category term="NLP"/><category term="conferences"/><category term="word2vec"/><category term="distributive_semantics"/><summary type="html"><![CDATA[Helsinki, August 2015]]></summary></entry></feed>